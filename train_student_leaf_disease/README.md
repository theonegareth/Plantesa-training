## Knowledge Distillation

Knowledge Distillation is a model compression technique where a smaller, simpler model (the "student") is trained to replicate the behavior of a larger, more complex model (the "teacher"). The student learns not only from the ground truth labels but also from the teacher's output, enabling it to achieve high accuracy with reduced computational resources.

_Source: [Keras Knowledge Distillation Example](https://keras.io/examples/vision/knowledge_distillation/)_

