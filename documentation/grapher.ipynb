{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e57f44",
   "metadata": {},
   "source": [
    "# Training and Validation Performance Analysis\n",
    "\n",
    "The following plots visualize the training and validation accuracy and loss over epochs. These graphs help to compare model performance on the training and validation sets, identify overfitting or underfitting, and assess the effectiveness of different hyperparameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c07744a",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d711410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Log data as a string\n",
    "log_data = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Regular expression to extract epoch, accuracy, loss, val_accuracy, and val_loss\n",
    "pattern = r\"Epoch (\\d+)/\\d+\\n.*accuracy: ([\\d.]+) - loss: ([\\d.]+) - val_accuracy: ([\\d.]+) - val_loss: ([\\d.]+)\"\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, log_data)\n",
    "\n",
    "# Convert matches to a list of dictionaries\n",
    "log_results = [\n",
    "  {\n",
    "    \"Epoch\": int(match[0]),\n",
    "    \"Accuracy\": float(match[1]),\n",
    "    \"Loss\": float(match[2]),\n",
    "    \"Val_Accuracy\": float(match[3]),\n",
    "    \"Val_Loss\": float(match[4]),\n",
    "  }\n",
    "  for match in matches\n",
    "]\n",
    "\n",
    "# Display the extracted data\n",
    "for result in log_results:\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics from log_results\n",
    "training_accuracy = [result['Accuracy'] for result in log_results]\n",
    "validation_accuracy = [result['Val_Accuracy'] for result in log_results]\n",
    "training_loss = [result['Loss'] for result in log_results]\n",
    "validation_loss = [result['Val_Loss'] for result in log_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_accuracy, label='Training Accuracy', marker='o')\n",
    "plt.plot(validation_accuracy, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_loss, label='Training Loss', marker='o')\n",
    "plt.plot(validation_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_Matrix = [[125, 0, 7, 0, 0, 1, 3, 0, 0, 0], [2, 87, 30, 3, 4, 7, 14, 2, 3, 0], [0, 5, 125, 0, 2, 2, 4, 1, 0, 0], [1, 3, 17, 119, 0, 4, 1, 0, 4, 0], [9, 4, 22, 7, 92, 1, 7, 2, 2, 0], [0, 1, 0, 0, 0, 158, 3, 0, 2, 0], [0, 3, 2, 1, 0, 28, 122, 0, 1, 3], [2, 0, 2, 0, 0, 4, 1, 139, 0, 0], [0, 0, 1, 0, 0, 1, 1, 0, 141, 0], [0, 0, 2, 0, 0, 5, 4, 0, 0, 151]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc31113",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classes = ['Bacterial_spot', 'Early_blight', 'Late_blight', 'Leaf_Mold',\n",
    "           'Septoria_leaf_spot', 'Spider_mites Two-spotted_spider_mite', 'Target_Spot',\n",
    "           'Tomato_Yellow_Leaf_Curl_Virus', 'Tomato_mosaic_virus', 'healthy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0fe81d",
   "metadata": {},
   "source": [
    "A confusion matrix is a table used to evaluate the performance of a classification model. It shows the number of correct and incorrect predictions for each class, helping to identify where the model is making errors and which classes are being confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=Classes, yticklabels=Classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1105a",
   "metadata": {},
   "source": [
    "A normalized confusion matrix shows the proportion of correct and incorrect predictions for each class, with values scaled so each row sums to 1. This makes it easier to compare model performance across classes, regardless of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "normalized_confusion_matrix = np.array(confusion_matrix) / np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(normalized_confusion_matrix, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=Classes, yticklabels=Classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
